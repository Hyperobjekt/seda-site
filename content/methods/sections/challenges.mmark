---
section: challenges
---
### Challenges Working with Proficiency Data

While there is a substantial amount of data from every state available in EDFacts, there are several key challenges when using it:

States provide only “proficiency data”: the count of students at each of the proficiency levels (sometimes called “achievement levels” or “performance levels”). The levels represent different degrees of mastery of the subject-specific grade-level material. Levels are defined by score “thresholds”, which are set by experts in the field. Scoring above or below different thresholds determines placement into a specific proficiency level. Common levels include “below basic”, “basic”, “proficient” and “advanced”. An example is shown below.

<br>

{.table-responsive .seda-table}

| Test Score 	| Proficiency Level 	| Description                                	|
|------------	|-------------------	|--------------------------------------------	|
| 0-50       	| Below Basic       	| Inadequate performance; minimal mastery    	|
| 51-75      	| Basic             	| Marginal performance; partial mastery      	|
| 76-90      	| Proficient        	| Satisfactory performance; adequate mastery 	|
| 91-100     	| Advanced          	| Superior performance; complete mastery     	|

<br>

<img src="/images/methods/image2.png" class="w-100" />

<br>

Most states use their own test and define “proficiency” in different ways, meaning that we cannot directly compare test results in one state to another. Proficient in one state/grade/year/subject is not comparable to proficient in another. 

Consider two states, State A and B that use same test, but set their own thresholds for proficiency.
<br><br>
##### State A: Higher Threshold for Proficiency

<img src="/images/methods/image2.png" class="w-100" />

<br>
##### State B: Lower Threshold for Proficiency

<img src="/images/methods/image3.png" class="w-100" />

<br>

Imagine both states had the same distribution of achievement, with 100 kids scoring a 55 on the exam, 200 scoring a 70, and 100 scoring an 85. Their assignment to proficiency categories would differ because of where the thresholds are placed.

{.table-responsive .seda-table}

|       	| Not Proficient 	|         	| Proficient 	|         	|
|-------	|----------------	|---------	|------------	|---------	|
| State 	| Level 1        	| Level 2 	| Level 3    	| Level 4 	|
| A     	| 0              	| 300     	| 100        	| 0       	|
| B     	| 0              	| 100     	| 200        	| 100     	|

In this example, State B looks more “proficient” than State A because 300 of their students score proficient compared to 100 students in State A. But, their achievement was identical! The bottom line: If you only use the proficiency data without accounting for where they thresholds for proficiency are places, you could end up with the wrong conclusion about which state performed better.

This problem is actually even more complicated that the example suggests because states use different tests with material of varying difficulty and scores reported on different scales. So, it is not even clear that we could compare their proficiency thresholds.

Even within a state, different tests are used in different grade levels. This means that, for example, we cannot compare the performance of students in 4th grade in one year to that of students in 5th grade in the next year. In other words, we cannot measure average growth.

States change the grade-level tests they use over time. This may result from changes in curricular standards; for example, the introduction of the Common Core State Standards led to many states adopting different tests. These test changes make it hard to compare average performance in one year to the next. In other words, we cannot recover reliable estimates of trends over time.


